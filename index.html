<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="COLD-Attack">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <link rel="icon" type="image/x-icon" href="./static/images/cold_attack_gpt4.webp">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="static/js/bulma-carousel.js"></script>
  <style>
    .results-carousel {
        width: 80%;       /* Set to your desired width */
        margin: 0 auto;   /* Center the carousel horizontally */
    }
  </style>

</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/guoxingang">Xingang Guo</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://yu-fangxu.github.io/">Fangxu Yu</a><sup>2*</sup>,</span>
            <span class="author-block">
              <a href="https://www.huan-zhang.com/">Huan Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/lianhuiqin/home">Lianhui Qin</a><sup>2</sup><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://binhu7.github.io/">Bin Hu</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Illinois Urbana-Champaign,</span>
            <span class="author-block"><sup>2</sup>University of California San-Diego</span>
            <span class="author-block"><sup>3</sup>Allen Institute for AI</span>
            <span class="author-block"><sup>*</sup><small>Equal Contribution</small></span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.08679"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Yu-Fangxu/COLD-Attack"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


  <!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Jailbreaks on large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study  controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent suffix attacks, but also allow us to address new controllable attack settings such as revising a user query adversarially with minimal paraphrasing, and inserting stealthy attacks in context with left-right-coherence. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section">
  <div class="container is-max-desktop">
     <h3>COLD-Attack Framework</h3>
  </div>
    <!-- Summary of the work -->
     <div class="content">
    <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/COLD_attack_diagram.png"
           style="width: 100%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
    </div>
    
    <!-- Explanation of the diagram -->
    <div class="content" style="margin-top: 20px;">
      <p>As illustrated in the above diagram, our COLD-Attack framework includes three main steps:</p>
      <ol>
        <li><strong>Energy function formulation:</strong> Specify energy functions properly to capture the attack constraints such as fluency, stealthiness, sentiment, and left-right-coherence.</li>
        <li><strong>Langevin dynamics sampling:</strong> Run Langevin dynamics recursively for specific steps to obtain a good energy-based model governing the adversarial attack logits.</li>
        <li><strong>Decoding process:</strong> Leverage an LLM-guided decoding process to convert the continuous logits into discrete text attacks.</li>
      </ol>
      <p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Summary of the work -->
    <div class="content">
      <h3>Attack Settings</h3>
      <p>
        The COLD-Attack framework unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios including:
      <ul>
          <li><strong>Fluent suffix attacks</strong>: standard attack setting which append the adversarial prompt to the original malicious user query.</li>
          <li><strong>Paraphrase attack with and without sentiment steering</strong>: revising a user query adversarially with minimal paraphrasing.</li>
          <li><strong>Attack with left-right-coherence</strong>: inserting stealthy attacks in context with left-right-coherence.</li>
      </ul>
      Here are some examples that generated by COLD-Attack:
      </p>
    </div>
    
    <!-- Large plot display -->
    <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/selected_samples_2.png"
           style="width: 100%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="./static/images/ex1.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="./static/images/ex2.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
      <!-- Your image here -->
      <img src="./static/images/ex3.png" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
      <!-- Your image here -->
      <img src="./static/images/ex4.png" alt="MY ALT TEXT"/>
      </div>
  </div>
</div>
</div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Summary of the work -->
    <div class="content">
      <h3>Experiment Results</h3>
      <strong>Fluent suffix attack</strong>
      <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/t1.png"
           style="width: 100%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
        </div>
        <div class="content has-text-justified">
        <ul>
          <li> COLD-Attack achieves comparable ASRs compared to the existing baseline methods such as GCG, AutoDAN-zhu, and AutoDAN-liu.
          <li> COLD-Attack stands out by achieving better stealthiness with lower PPL compared to all other methods.
        </ul>
       </div>
     
      <strong>Paraphrase attack</strong>
      <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/t2.png"
           style="width: 50%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
      </div>
        <div class="content has-text-justified">
        <ul>
          <li> COLD-Attack approach produces high-quality rephrasing.
          <li> COLD-Attack significantly outperforms GPT-4-based rephrased queries in terms of ASR.
          <li> Our paraphrase attack with sentiment control reveals that different LLMs exhibit varying susceptibilities to different sentiments.
        </ul>
       </div>
      <strong>Attack with left-right-coherence control</strong>
      <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/t3.png"
           style="width: 70%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
        </div>
         <div class="content has-text-justified">
        <ul>
          <li> COLD-Attack can effectively generate stealthy attacks that satisfy the left-right-coherence requirement.
          <li> COLD-Attack also allows the use of separate prompts to pose output constraints on the target LLMs.
        </ul>
      </p>
      </div>
   </div>
  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{guo2024cold,
  author    = {Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin},
  title     = {COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability},
  journal   = {arXiv preprint arXiv:2402.08679},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-6">
        <div class="content">
          <p>
            The source code of this webpage is based on the <a href="https://github.com/nerfies/nerfies.github.io/">
              Nerfies</a> project webpage.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
