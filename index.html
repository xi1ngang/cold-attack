<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="COLD-Attack">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="static/js/bulma-carousel.js"></script>
  <style>
    .results-carousel {
        width: 80%;       /* Set to your desired width */
        margin: 0 auto;   /* Center the carousel horizontally */
    }
  </style>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/cold_attack_gpt4.webp">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/script.js"></script>

</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/guoxingang">Xingang Guo</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://yu-fangxu.github.io/">Fangxu Yu</a><sup>2*</sup>,</span>
            <span class="author-block">
              <a href="https://www.huan-zhang.com/">Huan Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/lianhuiqin/home">Lianhui Qin</a><sup>2</sup><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://binhu7.github.io/">Bin Hu</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Illinois Urbana-Champaign,</span>
            <span class="author-block"><sup>2</sup>University of California San-Diego</span>
            <span class="author-block"><sup>3</sup>Allen Institute for AI</span>
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.08679"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Yu-Fangxu/COLD-Attack"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h3>COLD-Attack Framework</h3>
    <!-- Summary of the work -->
    <div class="content">
      <p>
        In this work, we present COLD-Attack, a novel attack mechanism that generates controllable adversarial prompts to jailbreak large language models (LLMs).
      </p>
    </div>
    
    <!-- Large plot display -->
    <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/COLD_attack_diagram.png"
           style="width: 100%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
    </div>
    
    <!-- Explanation of the diagram -->
    <div class="content" style="margin-top: 20px;">
      <p>As illustrated in the above diagram, our COLD-Attack framework includes three main steps:</p>
      <ol>
        <li><strong>Energy function formulation:</strong> Specify energy functions properly to capture the attack constraints such as fluency, stealthiness, sentiment, and left-right-coherence.</li>
        <li><strong>Langevin dynamics sampling:</strong> Run Langevin dynamics recursively for specific steps to obtain a good energy-based model governing the adversarial attack logits.</li>
        <li><strong>Decoding process:</strong> Leverage an LLM-guided decoding process to convert the continuous logits into discrete text attacks.</li>
      </ol>
      <p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h3>Attack Settings</h3>
    <!-- Summary of the work -->
    <div class="content">
      <p>
        The COLD-Attack framework unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios including:
      <ul>
          <li><strong>Fluent suffix attacks</strong> (standard attack setting which append the adversarial prompt to the original malicious user query).</li>
          <li><strong>Paraphrase attack with and without sentiment steering</strong> (revising a user query adversarially with minimal paraphrasing).</li>
          <li><strong>Attack with left-right-coherence</strong> (inserting stealthy attacks in context with left-right-coherence).</li>
      </ul>
      Here are some examples that generated by COLD-Attack:
      </p>
    </div>
    
    <!-- Large plot display -->
    <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/selected_samples_2.png"
           style="width: 100%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h3>Experiment Results</h3>
    <!-- Summary of the work -->
    <div class="content">
      <p>
      We evaluate the performance of COLD-Attack on four popular white-box LLMs: Vicuna-7b-v1.5 (Vicuna), Llama-2-7b-Chat-hf (Llama2), Guanaco-7b (Guanaco), and Mistral-7b-Instruct-v0.2 (Mistral). In addition, we use the following three main evaluation metrics:
        <ul>
          <li><strong>Attack Successful Rate (ASR):</strong>: the percentage of instructions that elicit corresponding harmful outputs using sub-string matching method.
          <li><strong>GPT-4 based ASR (ASR-G):</strong>: We develop a prompt template and utilize GPT-4 to assess whether a response accurately fulfills the malicious instruction. Based on our observations, ASR-G has shown higher correlation with human annotations, providing a more reliable measure of attack effectiveness.
          <li><strong>Perplexity (PPL):</strong>We use PPL to evaluate the fluency of the generated prompts and use Vicuna-7b to do the PPL calculation.
        </ul>
          To ensure the generated adversarial prompts meet specific criteria, we apply controls over various features, including sentiment and vocabulary. We evaluate how well these controls work using a metric called <strong>Succ</strong>, which represents the percentage of samples that successfully adhere to our set requirements. Additionally, a range of NLP-related evaluation metrics including <strong>BERTScore</strong>, <strong>BLEU</strong>, and <strong>ROUGE</strong> are applied to evaluate the quality of generated controllable attacks.      
      </p>
      <strong>Fluent suffix attack</strong>
      <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/t1.png"
           style="width: 100%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
      </div>
      <strong>Paraphrase attack</strong>
      <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/t2.png"
           style="width: 100%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
      </div>
      <strong>Left-right-coherence control</strong>
      <div style="text-align: center; margin-top: 20px;"> <!-- Center align the image container -->
      <img src="./static/images/t3.png"
           style="width: 100%; max-width: 800px; height: auto; display: block; margin: 0 auto;" <!-- Make the image responsive and centered -->
      </div>


<p>Please see more detailed evaluation results and discussions in our paper.</p>

    </div>
  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{guo2024cold,
  author    = {Guo, Xingang and Yu, Fangxu and Zhang, Huan and Qin, Lianhui and Hu, Bin},
  title     = {COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability},
  journal   = {arXiv preprint arXiv:2402.08679},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
